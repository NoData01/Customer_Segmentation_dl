# -*- coding: utf-8 -*-
"""Customer_Segmentation_dl.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0H7khhDjEpCSoc4Q2h1bzgWdHILC9YB
"""

# Edited on 22 June 2022

from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from module_customer_classes import Plot_graph, model_creation, Cramers_V
from sklearn.experimental import enable_iterative_imputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.impute import IterativeImputer
from sklearn.impute import KNNImputer

from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.layers import Input
from tensorflow.keras import Sequential
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import TensorBoard
import matplotlib.pyplot as plt
import scipy.stats as ss 
import seaborn as sns
import pandas as pd
import numpy as np
import datetime
import pickle
import os

#%% Statics
DATA_TRAIN_PATH = os.path.join(os.getcwd(),'Train.csv')
LE_PATH = os.path.join(os.getcwd(),'le.pkl')
log_dir = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
CUSTOMER_LOG_FOLDER_PATH = os.path.join(os.getcwd(),'logs',log_dir)
SS_FILE_NAME = os.path.join(os.getcwd(),'standard_scaler.pkl')
OHE_PATH = os.path.join(os.getcwd(),'ohe.pkl')
MODEL_PATH = os.path.join(os.getcwd(),'model.h5')

# EDA
#%% Step1 - Data Loading
df = pd.read_csv(DATA_TRAIN_PATH, na_values='')

#%% Step2 - Data Inspection/Visualization
df.head(10)
df.tail(10)
df.info()  # A NaN value is detected in the dataset 
df.describe().T

df.columns  # To view the column names (18 columns)

plt.figure(figsize=(25,10))
df.boxplot()  
# A noticeable outliers is detected in 'balance' and 'last contact duration'
# columns
plt.show()

categorical_column = ['job_type', 'marital', 'education', 'default', 
                      'housing_loan', 'personal_loan', 'communication_type', 
                      'day_of_month', 'month', 'prev_campaign_outcome', 
                      'term_deposit_subscribed']

continuous_column = ['customer_age', 'balance', 'last_contact_duration',
                     'num_contacts_in_campaign', 'num_contacts_prev_campaign']

# From the df.info(), we noticed that 'days_since_prev_campaign_contact' column
# have too many NaN value, and 'id' column is unnecessary column, 
# thus it will not be included.

pg = Plot_graph()
pg.plot_categorical(df,categorical_column)
pg.plot_continuous(df,continuous_column)
pg.groupby(df)

# From here we can see that those with blue-collar job type have the highest 
# number that are not doing the deposit subscribed.
# On the other hand, secondary education contribute the largest amount of 
# people that are not doing the deposit subscribed and this also same goes to 
# the married group

df.duplicated().sum() # 0 duplicated data is been detected

#%% Step3 - Data Cleaning
# 1) Drop the unnecassary column
df = df.drop(labels=['id', 'days_since_prev_campaign_contact'], axis=1)
# Since, the 'days_since_prev_campaign_contact' column have so many NaN values, 
# decided to drop it.

column_names = df.columns # new column names

# 2) Drop the duplicated data
# From the df.duplicated().sum(), it seems no duplicated data need to be removed

# 3) Deals with NaNs
df.isna().sum()

# If we want to use KNN or MICE, need to do label encoder first wihtout touching 
# the NaN

for cat in categorical_column:
  le = LabelEncoder()
  temp = df[cat]
  temp[temp.notnull()] = le.fit_transform(temp[temp.notnull()])
  with open(LE_PATH,'wb') as file:
    pickle.dump(le,file)


# df will use for simple imputer
df_backup1 = df.copy() #will use for testing KNN
df_backup2 = df.copy() #will use for testing MICE

# Impute NaN using median
for cat in categorical_column:
  df[cat] = df[cat].fillna(df[cat].mode()[0])

for con in continuous_column:
  df[con] = df[con].fillna(df[con].median())

df.isna().sum() # check again if there is NaN anymore
df.describe().T

# KNNImputer
knn_imputer = KNNImputer(n_neighbors=5, metric='nan_euclidean')
knn_imputed_data = knn_imputer.fit_transform(df_backup1)
df_backup1 = pd.DataFrame(knn_imputed_data)
df_backup1.columns = column_names
for i in column_names:
    df_backup1.loc[:,i] = np.floor(df_backup1.loc[:,i]).astype('int')

df_backup1.isna().sum() # check again if there is NaN anymore
df_backup1.describe().T 

# MICE
it_imputer = IterativeImputer()
it_imputed_data = it_imputer.fit_transform(df_backup2)
df_backup2 = pd.DataFrame(it_imputed_data)
df_backup2.columns = column_names

df_backup2.isna().sum() # check again if there is NaN anymore
df_backup2.describe().T

# Choose KNN Imputer for the best imputer(df_backup2)

#%% Step4 - Features Selection
# (Categorical vs Categorical)
# Cramer's V
# cv = Cramers_V()
# for cat in categorical_column:
#     print(cat)
#     confussion_matrix = pd.crosstab(df_backup1[cat],
#                                  df_backup1['term_deposit_subscribed']).to_numpy()
#     print(str(cv.cramers_corrected_stat(confussion_matrix)))

# Since all of the feature in categorical have the correlation >= 0.5, 
# Then no feature will be selected.

# (Continuous vs Categorical)
# Logistic Regression
# for con in continuous_column:
#     lr = LogisticRegression()
#     lr.fit(np.expand_dims(df_backup1[con],axis=-1),
#            df_backup1['term_deposit_subscribed'])
#     print(con + '=' + str(lr.score(np.expand_dims(df_backup1[con],axis=-1),
#                                    df_backup1['term_deposit_subscribed'])))

# Here, 'customer_age','balance','last_contact_duration',
# 'num_contacts_in_campaign', and 'num_contacts_prev_campaign' have 
# achieved more than 50% accuracy which are 89%, 89%, 90%, 89% and 
# 89% respectively. Choose all of them as they achieved more than 50%

# With feature selection
# X = df_backup1.loc[:,['customer_age','balance','last_contact_duration',
#                       'num_contacts_in_campaign','num_contacts_prev_campaign']]
# y = df_backup1['term_deposit_subscribed']

# With no feature selection 
X = df_backup1.drop(labels=['term_deposit_subscribed'],axis=1)
y = df_backup1['term_deposit_subscribed']

#%% Step5 - Data Preprocessing
# Features Scalling
ss = StandardScaler()
X = ss.fit_transform(X)

# Need to save ss model
with open(SS_FILE_NAME,'wb') as file:
  pickle.dump(ss,file)

#OneHotEncoder
ohe = OneHotEncoder(sparse=False)
y = ohe.fit_transform(np.expand_dims(y,axis=-1))

with open(OHE_PATH,'wb') as file:
  pickle.dump(ohe,file)

#Train test split
X_train,X_test,y_train,y_test = train_test_split(X,y,
                                                 test_size=0.3,
                                                 random_state=123)

#%% Model development
nb_features = np.shape(X_train)[1:]
nb_classes = len(np.unique(y,axis=0))

mc = model_creation()
model = mc.model_development(nb_features,nb_classes,num_node=128,dropout=0.3)

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics = ['acc'])

plot_model(model, to_file='model_plot.png', show_shapes=True, 
           show_layer_names=True)

# Callbacks
tensorboard_callback = TensorBoard(log_dir=CUSTOMER_LOG_FOLDER_PATH)
early_stopping_callback = EarlyStopping(monitor='Loss',patience=3)

# Model training
hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),
                batch_size=128, epochs=100,
                callbacks=[tensorboard_callback,early_stopping_callback])

with open(MODEL_PATH,'wb') as file:
  pickle.dump(model,file)

#%%
hist.history.keys()

plt.figure()
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.legend(['training_loss','validation loss'])
plt.show()

plt.figure()
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.legend(['training_accuracy','validation accuracy'])
plt.show()
# From the graph it shows that the model experiences not overfitting and not 
# also underfitting with accuracy aaround 90%

#%% Model evaluation (after training the data)
results = model.evaluate(X_test,y_test)
print(results)
y_true = np.argmax(y_test,axis=1)
y_pred = np.argmax(model.predict(X_test),axis=1)

cr = classification_report(y_true,y_pred)
cm = confusion_matrix(y_true,y_pred)

print(cr)
print(cm)

label = ['No Subscribed','Subscribed']
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=label)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs